{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on https://github.com/vahidk/EffectiveTensorflow.  \n",
    "___\n",
    "\n",
    "### Doing computation in tensorflow\n",
    "Tensorflow give us tensor which like a placeholder for our variable.\n",
    "A tf.Tensor object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of tf.Tensor objects.  \n",
    "In order to compute the value of the tensor we need to create a session and evaluate it using Session.run() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is z  Tensor(\"MatMul_1:0\", shape=(10, 10), dtype=float32)\n",
      "[[  0.98684156   2.863435    -1.6667035   -0.8425131   -1.7591304\n",
      "    0.2621978   -0.4838299    3.8139358    1.0740364   -0.4735332 ]\n",
      " [  0.1721021    0.39912608  -1.2777555   -0.7923951   -0.9775923\n",
      "   -0.16850671  -4.5594683   -1.2539387   -0.04540014   0.36295214]\n",
      " [  3.0229654   -1.1301382    5.3861213   -3.376586     2.1859884\n",
      "   -3.6515145   -0.59590495  -2.1741357    2.416408     2.274809  ]\n",
      " [ -5.1114025   -6.185856     6.9744883   -2.6803067   -0.9619111\n",
      "   -0.04638492   4.5675793    2.4024372    2.6572094   -0.8861291 ]\n",
      " [  1.7131717   -0.06711298   6.6898293   -5.775112    -1.9994012\n",
      "    3.7297204   -4.26378     -1.8154532  -13.651993    -2.3754747 ]\n",
      " [ -1.0223211    6.627446    -1.2646064    0.48809242   1.3152277\n",
      "   -0.2615651   -0.6642335    2.4925654    3.3332803    2.2760768 ]\n",
      " [ -0.45090076   1.2121685    5.0687294   -2.3027318   -3.1910555\n",
      "    3.8441775   -1.0027664   -1.3575561   -4.9749346    2.625601  ]\n",
      " [ -4.541996    -3.6037683    2.3309147    0.5060692   -0.74432814\n",
      "   -0.89669335   4.910463     1.7558461    5.7068963    0.6575185 ]\n",
      " [ -1.1290994   -4.664427     0.49854994   0.4620694   -3.0550406\n",
      "   -4.3819003    2.1635559    1.0874777    6.3989925    0.05369008]\n",
      " [ -1.1586757   -0.7501813    1.4479549    1.0049822    3.0842829\n",
      "    1.3063365    2.5521402   -2.4664736   -1.6802831    0.35960567]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "y = tf.random_normal([10, 10])\n",
    "z = tf.matmul(x, y)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "z_val = sess.run(z)\n",
    "print(\"this is z \",z)\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic computation\n",
    "Symbolic computation is a very different way of programming.\n",
    "\n",
    "Classical programming defines variables that hold values, and operations to modify their values.\n",
    "\n",
    "In symbolic programming, it’s more about building a graph of operations, that will be compiled later for execution. Such an architecture enables the code to be compiled and executed indifferently on CPU or GPU for example. Symbols are an abstraction that does not require to know where it will be executed.\n",
    "\n",
    "let's say.  \n",
    "- $f(x) = 5x^2 + 3$  \n",
    "Parametric function  \n",
    "- $g(x, w) = w_{0} x^2 + w_{1} x + w_{2}$  \n",
    "our goal is then to find the latent parameters w such that   \n",
    "- $g(x, w) ≈ f(x)$  \n",
    "his can be done by minimizing the following loss function:  \n",
    "- $L(w) = ∑ (f(x) - g(x, w))^2$   \n",
    "   \n",
    "To solve this problem we can use stocasthic gradient decent\n",
    "which is simply update the value of w by moving it on it's gradient by\n",
    "computing derivative of $L(w)$ and move it to opposite direction (minimizing the loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[4.9946995e+00],\n",
      "       [5.1860115e-04],\n",
      "       [3.3083560e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder are used to feed value from python to tensorflow ops. We define two placeholders\n",
    "# One for input feature x and one for output feature y\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Assuming we know that the desired function is a polynomial 2nd degree, \n",
    "# we allocate a vector of size 3 to hold the coefficient. The variable will \n",
    "# be automatically initiated with random noise\n",
    "## tf.get_variable just set variable which a name, so it can be called on different variable but with \n",
    "## same variable w\n",
    "w = tf.get_variable(\"w\", shape=[3,1]) #, initializer = tf.random_normal_initializer())\n",
    "\n",
    "# we define yhat which is our estimate of y\n",
    "## tf.square compute x*x\n",
    "## tf.ones_like fill x with 1\n",
    "## tf.st [w,w,w] * [x^2, x , 1] **I still don't understand why they put squeeze in here**\n",
    "f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "\n",
    "# The loss is L2 distance from estimate y and predicted value y\n",
    "# we also added shrinkage item (L1 norm)\n",
    "loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n",
    "\n",
    "# We use the Adam optimizer with learning rate set to 0.1 to minizmise the loss.\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val \n",
    "\n",
    "sess = tf.Session()\n",
    "# Since we are using varibles we first need to initialize them.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "    _, loss_val = sess.run([train_op, loss],{x: x_val, y: y_val})\n",
    "#     print(loss_val)\n",
    "print(sess.run([w]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the result above is the predicted value of w\n",
    "___\n",
    "### Understanding static and dynamic shapes\n",
    "#### Static shapes\n",
    "Static types is defined shapes.\n",
    "e.g array with element size 255\n",
    "#### Dynamic shapes\n",
    "If you want to feed the neural network with a number of batch\n",
    "that you can modify, then just define the first dimension as zero or -1. If you look the code below, the first dimension will be deterimend dynamically during Session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, [None,128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the dynamic shape of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static shape of tensor can be set with Tensor.set_shape() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.set_shape([32, 128])\n",
    "a.set_shape([None,, 128]) # first dimension of a will bedetermined dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reshape a give tensor dynamically using tf.reshape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(a, [32,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'unstack:1' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(tf.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "   static_shape = tensor.shape.as_list()\n",
    "   dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "   dims = [s[1] if s[0] is None else s[0]\n",
    "             for s in zip(static_shape, dynamic_shape)]\n",
    "   return dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "shape = get_shape(b)\n",
    "b = tf.reshape(b, [shape[0], shape[1] * shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
